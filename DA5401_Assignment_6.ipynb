{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b384eaa3-dc0c-4668-a4a6-6f9f387b1804",
   "metadata": {},
   "source": [
    "# Assignment 6: Imputation via Regression for Missing Data\n",
    "\n",
    "In this assignment, I explored the challenge of handling missing data in the **UCI Credit Card Default Clients** dataset, focusing on how different **imputation strategies** influence downstream classification performance. Artificially introducing **Missing At Random (MAR)** values in key numerical attributes such as AGE and BILL_AMT, I implemented three distinct approaches — **median imputation, linear regression imputation, and non-linear regression imputation** (KNN) — alongside a listwise deletion method. Each cleaned dataset was used to train a Logistic Regression classifier, and the resulting performance metrics (Accuracy, Precision, Recall, F1-score) were compared to assess imputation efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818c6e2-2d43-4997-b0d2-32c286aecc03",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251b6da-6ecf-4a79-af8d-ddd4489d79fe",
   "metadata": {},
   "source": [
    "#### Importing the necessary libraries\n",
    "We start by importing the required libraries ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c075138-db5b-40e2-bbd7-f2ade501a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, make_scorer, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49700bb-63bc-4a11-a92f-ec49b8d41002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6e04b-cd73-4ecb-bf57-b11f3d10c2d0",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4ed6a-9adb-4521-81e0-8722ef05bb62",
   "metadata": {},
   "source": [
    "We load the data into pandas DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f149ae8f-81bb-4d47-9aaa-a984066ea905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('UCI_Credit_Card.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c8ed5e-1477-42f1-be3a-75a1e1c11f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f21f3f-7802-4c07-bbcc-11a97aba53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'default.payment.next.month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24d4736-4d15-4ec6-a37f-0d3472bbe562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (30000, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8cb66ab-9bd8-4b83-bc20-16b7488a953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                            0\n",
       "LIMIT_BAL                     0\n",
       "SEX                           0\n",
       "EDUCATION                     0\n",
       "MARRIAGE                      0\n",
       "AGE                           0\n",
       "PAY_0                         0\n",
       "PAY_2                         0\n",
       "PAY_3                         0\n",
       "PAY_4                         0\n",
       "PAY_5                         0\n",
       "PAY_6                         0\n",
       "BILL_AMT1                     0\n",
       "BILL_AMT2                     0\n",
       "BILL_AMT3                     0\n",
       "BILL_AMT4                     0\n",
       "BILL_AMT5                     0\n",
       "BILL_AMT6                     0\n",
       "PAY_AMT1                      0\n",
       "PAY_AMT2                      0\n",
       "PAY_AMT3                      0\n",
       "PAY_AMT4                      0\n",
       "PAY_AMT5                      0\n",
       "PAY_AMT6                      0\n",
       "default.payment.next.month    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786a1c0-a920-4ea9-88df-6f150193b8b2",
   "metadata": {},
   "source": [
    "We can see that there are no null values, so will now introduce some null values  to simulate a real-world scenario with a substantial missing data problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5983f8-3fa9-4110-a606-37ebf686cea8",
   "metadata": {},
   "source": [
    "**Introduce MAR:**\n",
    "\n",
    "We'll introduce missing values in two numerical columns: AGE and BILL_AMT1. We will make the missingness Missing At Random (MAR) by making the missing probability depend on LIMIT_BAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185e2026-e6a2-42d6-86d4-4fcbc791b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduced 2238 missing values in AGE (target ~7.0%)\n",
      "Introduced 2282 missing values in BILL_AMT1 (target ~7.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AGE          0.074600\n",
       "BILL_AMT1    0.076067\n",
       "Name: missing_fraction, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = ['AGE', 'BILL_AMT1']\n",
    "\n",
    "def introduce_mar(df, cols, frac=0.07, condition_col='LIMIT_BAL', random_state=RANDOM_STATE):\n",
    "    \"\"\"Introduce MAR by making missingness depend on a condition column.\n",
    "    frac is the *approximate* overall fraction of values to be missing in each column.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    df = df.copy()\n",
    "    cond_med = df[condition_col].median()\n",
    "    for col in cols:\n",
    "        # probability depends on whether condition column is above median\n",
    "        p_high = min(0.02 + frac*1.5, 0.5)\n",
    "        p_low = max(0.005, frac*0.4)\n",
    "        probs = np.where(df[condition_col] > cond_med, p_high, p_low)\n",
    "        mask = rng.rand(len(df)) < probs\n",
    "        df.loc[mask, col] = np.nan\n",
    "        print(f'Introduced {mask.sum()} missing values in {col} (target ~{frac*100:.1f}%)')\n",
    "    return df\n",
    "\n",
    "# Make a copy to preserve original\n",
    "df_missing = introduce_mar(df, candidates, frac=0.07)\n",
    "\n",
    "# Quick missing summary\n",
    "missing_summary = df_missing[candidates].isnull().mean().rename('missing_fraction')\n",
    "missing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e8de4-884e-409e-b6c0-1ed68ec834e0",
   "metadata": {},
   "source": [
    "We have introduced missing values in AGE and BILL_AMT1 column so we can proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f85f03f-da54-493b-854b-4bb3dc2912db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>167484.322667</td>\n",
       "      <td>1.603733</td>\n",
       "      <td>1.853133</td>\n",
       "      <td>1.551867</td>\n",
       "      <td>35.485500</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>-0.133767</td>\n",
       "      <td>-0.166200</td>\n",
       "      <td>-0.220667</td>\n",
       "      <td>...</td>\n",
       "      <td>43262.948967</td>\n",
       "      <td>40311.400967</td>\n",
       "      <td>38871.760400</td>\n",
       "      <td>5663.580500</td>\n",
       "      <td>5.921163e+03</td>\n",
       "      <td>5225.68150</td>\n",
       "      <td>4826.076867</td>\n",
       "      <td>4799.387633</td>\n",
       "      <td>5215.502567</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8660.398374</td>\n",
       "      <td>129747.661567</td>\n",
       "      <td>0.489129</td>\n",
       "      <td>0.790349</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>9.217904</td>\n",
       "      <td>1.123802</td>\n",
       "      <td>1.197186</td>\n",
       "      <td>1.196868</td>\n",
       "      <td>1.169139</td>\n",
       "      <td>...</td>\n",
       "      <td>64332.856134</td>\n",
       "      <td>60797.155770</td>\n",
       "      <td>59554.107537</td>\n",
       "      <td>16563.280354</td>\n",
       "      <td>2.304087e+04</td>\n",
       "      <td>17606.96147</td>\n",
       "      <td>15666.159744</td>\n",
       "      <td>15278.305679</td>\n",
       "      <td>17777.465775</td>\n",
       "      <td>0.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7500.750000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2326.750000</td>\n",
       "      <td>1763.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>8.330000e+02</td>\n",
       "      <td>390.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>117.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19052.000000</td>\n",
       "      <td>18104.500000</td>\n",
       "      <td>17071.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1800.00000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22500.250000</td>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54506.000000</td>\n",
       "      <td>50190.500000</td>\n",
       "      <td>49198.250000</td>\n",
       "      <td>5006.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4505.00000</td>\n",
       "      <td>4013.250000</td>\n",
       "      <td>4031.500000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891586.000000</td>\n",
       "      <td>927171.000000</td>\n",
       "      <td>961664.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>896040.00000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
       "count  30000.000000    30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean   15000.500000   167484.322667      1.603733      1.853133      1.551867   \n",
       "std     8660.398374   129747.661567      0.489129      0.790349      0.521970   \n",
       "min        1.000000    10000.000000      1.000000      0.000000      0.000000   \n",
       "25%     7500.750000    50000.000000      1.000000      1.000000      1.000000   \n",
       "50%    15000.500000   140000.000000      2.000000      2.000000      2.000000   \n",
       "75%    22500.250000   240000.000000      2.000000      2.000000      2.000000   \n",
       "max    30000.000000  1000000.000000      2.000000      6.000000      3.000000   \n",
       "\n",
       "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      35.485500     -0.016700     -0.133767     -0.166200     -0.220667   \n",
       "std        9.217904      1.123802      1.197186      1.196868      1.169139   \n",
       "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
       "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
       "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
       "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
       "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
       "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
       "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
       "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
       "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
       "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
       "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
       "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
       "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
       "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default.payment.next.month  \n",
       "count   30000.000000                30000.000000  \n",
       "mean     5215.502567                    0.221200  \n",
       "std     17777.465775                    0.415062  \n",
       "min         0.000000                    0.000000  \n",
       "25%       117.750000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4000.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f655e-5257-419b-8442-3d30f43d967a",
   "metadata": {},
   "source": [
    "### 2. Imputation Strategy 1: Simple Imputation (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a64508-c362-42f2-872d-642d94efd3d7",
   "metadata": {},
   "source": [
    "The median is preferred over the mean because median is robust to outliers and skewed distributions. When a feature's distribution is non-normal or contains extreme values, the **median provides a central location less influenced by those extremes** than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105a43b7-503f-432f-b91e-27228ae34c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled AGE missing with median=34.0\n",
      "Filled BILL_AMT1 missing with median=22614.5\n"
     ]
    }
   ],
   "source": [
    "df_A = df_missing.copy()\n",
    "\n",
    "for col in candidates:\n",
    "    median = df_A[col].median()\n",
    "    df_A[col] = df_A[col].fillna(median)\n",
    "    print(f'Filled {col} missing with median={median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f125b-a690-4282-b130-35461bfe6a84",
   "metadata": {},
   "source": [
    "### 3. Imputation Strategy 2: Regression Imputation (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab9137-5c7d-49fc-a77d-236c40a12c81",
   "metadata": {},
   "source": [
    "**Create a dataset with one missing column**\n",
    "\n",
    "We are using the df_missing dataset then filling the BILL_AMT1 column as it was in original dataset sothat now one **AGE** column contains missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d6dfad7-9959-48c9-8dd3-85d9c15e78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_missing = df_missing.copy()\n",
    "df_one_missing['BILL_AMT1'] = df['BILL_AMT1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a500eed-c6e4-461e-8c92-e4b5d4a2df31",
   "metadata": {},
   "source": [
    "We will impute one column with a `linear regression` model using the other features as predictors. We are picking `AGE` as the column to impute with regression.\n",
    "\n",
    "**Assumption**: Regression imputation assumes Missing At Random (MAR): that the probability of missingness depends on observed data (here `LIMIT_BAL`) but not on the unobserved (missing) values themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55cfa84-9c2a-4fae-b3ad-27098c7ce582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression features count: 23\n",
      "Linear regression imputation done. Imputed rows: 2238\n"
     ]
    }
   ],
   "source": [
    "col_to_impute = 'AGE'\n",
    "exclude = [col_to_impute, target_col]\n",
    "\n",
    "features = [c for c in df_one_missing.columns if c not in exclude]\n",
    "print('Regression features count:', len(features))\n",
    "\n",
    "# Separate rows\n",
    "not_missing_mask = df_one_missing[col_to_impute].notnull()\n",
    "X_train = df_one_missing.loc[not_missing_mask, features]\n",
    "y_train = df_one_missing.loc[not_missing_mask, col_to_impute]\n",
    "X_pred = df_one_missing.loc[~not_missing_mask, features]\n",
    "\n",
    "# Fit linear regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds_lin = lin_reg.predict(X_pred)\n",
    "\n",
    "# Clip unrealistic values (ages < 21 are unlikely in credit dataset)\n",
    "preds_lin = np.clip(preds_lin, a_min=21, a_max=None)\n",
    "\n",
    "# Create Dataset B\n",
    "df_B = df_one_missing.copy()\n",
    "df_B.loc[~not_missing_mask, col_to_impute] = preds_lin\n",
    "\n",
    "print('Linear regression imputation done. Imputed rows:', len(preds_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e98a1-32a8-4aa9-bf42-e4156bef0daf",
   "metadata": {},
   "source": [
    "### 4. Imputation Strategy 3: Regression Imputation (Non-Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d97070-64bb-4fc5-a11e-55dca47c64fa",
   "metadata": {},
   "source": [
    "We'll use `KNeighborsRegressor` (a non-linear method) to predict the same column (`AGE`). KNN can model local non-linear relationships and often works well when similar records exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf27a1-1c9c-414c-93af-002f5955ec6d",
   "metadata": {},
   "source": [
    "**Finding the best parameters for KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7affada-34cd-496d-8b71-f66edab4622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'metric': 'minkowski', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best CV MSE: 85.53912030992535\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': range(1, 31, 2),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski']  \n",
    "}\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "grid = GridSearchCV(\n",
    "    knn,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV MSE:\", -grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f111a-3089-481b-85f9-92cd863fa788",
   "metadata": {},
   "source": [
    "**Refit using the best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13888dc-9a8e-45a0-ba26-6bd3112252a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized KNN regression imputation done. Imputed rows: 2238\n"
     ]
    }
   ],
   "source": [
    "best_knn = grid.best_estimator_\n",
    "\n",
    "# Predict missing AGE\n",
    "preds_knn = best_knn.predict(X_pred)\n",
    "\n",
    "# Clip unrealistic values (ages < 21 are unlikely in credit dataset)\n",
    "preds_knn = np.clip(preds_knn, a_min=21, a_max=None)\n",
    "\n",
    "# Create Dataset C\n",
    "df_C = df_one_missing.copy()\n",
    "df_C.loc[~not_missing_mask, col_to_impute] = preds_knn\n",
    "\n",
    "print('Optimized KNN regression imputation done. Imputed rows:', len(preds_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39edd21-24d4-4274-a017-6e8c7f744810",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16af0ae-34cf-4e88-8259-99c8e858108c",
   "metadata": {},
   "source": [
    "**Create Dataset D (Listwise Deletion)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be083e5-fe86-4da3-93b0-8c66555c1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset D (listwise deletion) shape: (25705, 25)\n"
     ]
    }
   ],
   "source": [
    "df_D = df_missing.dropna().copy()\n",
    "print('Dataset D (listwise deletion) shape:', df_D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a683d-81bc-47bd-b7a3-aa269094ac97",
   "metadata": {},
   "source": [
    "### 1. Data Split, 2. Classifier Setup and 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5e218-a162-457a-beab-f7512020c471",
   "metadata": {},
   "source": [
    "**Common function to prepare X,y, split, scale, fit logistic regression, evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4af2e3-7d2d-48bb-86d5-c4096c17613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "def train_and_evaluate(df_in, dataset_name, features_exclude=['ID']):\n",
    "    \"\"\"\n",
    "    Train and evaluate a tuned Logistic Regression model using GridSearchCV.\n",
    "    \"\"\"\n",
    "    df_local = df_in.copy()\n",
    "\n",
    "    # Define X, y\n",
    "    drop_cols = [c for c in ['ID', target_col] if c in df_local.columns]\n",
    "    X = df_local.drop(columns=drop_cols)\n",
    "    y = df_local[target_col].astype(int)\n",
    "\n",
    "    # Train-test split (stratify if both classes present)\n",
    "    stratify_arg = y if len(np.unique(y)) > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_arg\n",
    "    )\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Logistic Regression with GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    base_clf = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1_weighted',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_clf = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate best model\n",
    "    y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision_pos': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall_pos': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1_pos': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'F1_macro': report['macro avg']['f1-score'] if 'macro avg' in report else np.nan,\n",
    "        'F1_weighted': report['weighted avg']['f1-score'] if 'weighted avg' in report else np.nan,\n",
    "        'n_train': X_train.shape[0],\n",
    "        'n_test': X_test.shape[0],\n",
    "    }\n",
    "\n",
    "    print(f\"\\n {dataset_name}- Best Parameters: {best_params}\")\n",
    "\n",
    "    return metrics, report_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea5921-203a-4207-a2fb-7dbfb3d2c639",
   "metadata": {},
   "source": [
    "**Evaluate models on datasets A, B, C, D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61ac8098-f65d-40a4-94f8-8a334ab99ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset A-Baseline\n",
      "\n",
      " A-Baseline- Best Parameters: {'C': 0.001, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      4673\n",
      "           1       0.49      0.52      0.50      1327\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.68      0.68      0.68      6000\n",
      "weighted avg       0.78      0.77      0.78      6000\n",
      "\n",
      "Training on dataset B-Linear\n",
      "\n",
      " B-Linear- Best Parameters: {'C': 0.001, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      4673\n",
      "           1       0.49      0.52      0.50      1327\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.68      0.68      0.68      6000\n",
      "weighted avg       0.78      0.77      0.78      6000\n",
      "\n",
      "Training on dataset C-Non-Linear\n",
      "\n",
      " C-Non-Linear- Best Parameters: {'C': 0.001, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      4673\n",
      "           1       0.49      0.52      0.50      1327\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.68      0.68      0.68      6000\n",
      "weighted avg       0.78      0.77      0.78      6000\n",
      "\n",
      "Training on dataset D-Listwise Deletion\n",
      "\n",
      " D-Listwise Deletion- Best Parameters: {'C': 0.001, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.51      0.64      3963\n",
      "           1       0.31      0.73      0.43      1178\n",
      "\n",
      "    accuracy                           0.56      5141\n",
      "   macro avg       0.59      0.62      0.54      5141\n",
      "weighted avg       0.74      0.56      0.60      5141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for df_use, name in [(df_A, 'A-Baseline'), (df_B, 'B-Linear'), (df_C, 'C-Non-Linear'), (df_D, 'D-Listwise Deletion')]:\n",
    "    print('Training on dataset', name)\n",
    "    metrics, rep = train_and_evaluate(df_use, name)\n",
    "    print(rep)\n",
    "    results.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783911c8-05f1-4a70-8646-711a6f09bfb0",
   "metadata": {},
   "source": [
    "Performance on dataset A, B and C is same but dataset D is less efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8547ebb-f72d-4418-87b5-b248d7990e9f",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e2abf-5159-4142-8573-a4e3e71bed09",
   "metadata": {},
   "source": [
    "### 1. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dcfeac4-74dc-44f9-907a-79d942fed809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_pos</th>\n",
       "      <th>Recall_pos</th>\n",
       "      <th>F1_pos</th>\n",
       "      <th>F1_macro</th>\n",
       "      <th>F1_weighted</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A-Baseline</th>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.489650</td>\n",
       "      <td>0.516956</td>\n",
       "      <td>0.502933</td>\n",
       "      <td>0.678343</td>\n",
       "      <td>0.776163</td>\n",
       "      <td>24000</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-Linear</th>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.489650</td>\n",
       "      <td>0.516956</td>\n",
       "      <td>0.502933</td>\n",
       "      <td>0.678343</td>\n",
       "      <td>0.776163</td>\n",
       "      <td>24000</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-Non-Linear</th>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.489650</td>\n",
       "      <td>0.516956</td>\n",
       "      <td>0.502933</td>\n",
       "      <td>0.678343</td>\n",
       "      <td>0.776163</td>\n",
       "      <td>24000</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D-Listwise Deletion</th>\n",
       "      <td>0.562926</td>\n",
       "      <td>0.308902</td>\n",
       "      <td>0.733447</td>\n",
       "      <td>0.434717</td>\n",
       "      <td>0.539223</td>\n",
       "      <td>0.595836</td>\n",
       "      <td>20564</td>\n",
       "      <td>5141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision_pos  Recall_pos    F1_pos  F1_macro  \\\n",
       "Dataset                                                                        \n",
       "A-Baseline           0.774000       0.489650    0.516956  0.502933  0.678343   \n",
       "B-Linear             0.774000       0.489650    0.516956  0.502933  0.678343   \n",
       "C-Non-Linear         0.774000       0.489650    0.516956  0.502933  0.678343   \n",
       "D-Listwise Deletion  0.562926       0.308902    0.733447  0.434717  0.539223   \n",
       "\n",
       "                     F1_weighted  n_train  n_test  \n",
       "Dataset                                            \n",
       "A-Baseline              0.776163    24000    6000  \n",
       "B-Linear                0.776163    24000    6000  \n",
       "C-Non-Linear            0.776163    24000    6000  \n",
       "D-Listwise Deletion     0.595836    20564    5141  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).set_index('Dataset')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725b54a-98a8-4566-8a6a-9208693842c1",
   "metadata": {},
   "source": [
    "All three imputation methods (A–C) achieved **identical performance**, while listwise deletion (D) showed a clear drop in every metric except recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72978cac-37f1-4d87-b565-90a51f86c66d",
   "metadata": {},
   "source": [
    "### 2. Efficacy Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8e4e5-9db7-4919-aaaf-5553e2bf4f44",
   "metadata": {},
   "source": [
    "**a) Trade-off: Listwise Deletion vs Imputation:**\n",
    "- **Listwise Deletion**: This method discards all records containing missing values, reducing the training size from 24,000 to 20,564 samples.\n",
    "Despite a higher recall (0.73), its **F1 Weighted (0.596)** and accuracy (0.56) are far lower due to **loss of data diversity** and **class imbalance amplification**.\n",
    "The model overpredicts the positive class to compensate, harming precision.\n",
    "\n",
    "- **Imputation methods** (A, B, C) retain full dataset that means they will have better generalization due to larger sample size and preserved variability.\n",
    "\n",
    "Hence, **Model D** performs slightly worse because it trains on fewer, less representative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794f9f7-39db-4566-850e-ec439b39d6fc",
   "metadata": {},
   "source": [
    "**b) Linear vs Non-linear Regression Imputation:**\n",
    "\n",
    "- Both **B (Linear)** and **C (Non-linear)** achieve nearly identical F1-scores.\n",
    "- This implies the imputed variable (AGE) shows **weak or roughly linear dependence** on the other features.\n",
    "- The non-linear KNN did not capture additional structure: possibly because other predictors (like LIMIT_BAL, BILL_AMT) have only moderate correlations with AGE, and KNN regression adds noise when relationships are weak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d323e1-5cdc-4af3-a6ca-cd6c95037c88",
   "metadata": {},
   "source": [
    "**c) Recommendation:**\n",
    "- **Median or Linear Regression Imputation:**\n",
    "    - Both achieve the **highest Weighted F1 (0.776)** and accuracy (0.774).\n",
    "    - They are efficient, stable, and conceptually appropriate for MAR (Missing At Random) data.\n",
    "    - Linear imputation may be favored when features have a clear linear structure; otherwise, median imputation is a safe and interpretable baseline.\n",
    "- **Non-linear Regression Imputation:**\n",
    "    - Adds complexity without gain here we can use it for cases with stronger non-linear dependencies or higher missingness.\n",
    "- **Listwise Deletion:**\n",
    "    - Not recommended as it causes substantial data loss and lower model performance.\n",
    "\n",
    "#### Conclusion:\n",
    "Considering both classification metrics (Weighted F1 and Accuracy) and conceptual reasoning, **Median or Linear Regression Imputation** offer the best trade-off between simplicity, robustness, and predictive performance.\n",
    "Non-linear imputation provides no benefit for this dataset, while listwise deletion should be avoided due to its severe information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a18168-fbd0-4f37-8a6a-aac90209ed8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
